# -*- coding: utf-8 -*-
"""RAHF_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQSm0-C9TBWVDIHHyqgtfpoYC42esbTs
"""

import torch
import torch.nn as nn

from transformers import AutoProcessor, AutoModel, AutoConfig

from .basic_components import Residual, Attention, FeedForward
import torch.nn.functional as F
# import sys
#   sys.path.append("/home/test/junlin/Eval/EVA-CLIP/rei")
from .resnet import resnet50
class VisionTransformer(nn.Module):
  def __init__(self, pretrained_model, freeze, new_position_embedding_weight):
    super(VisionTransformer, self).__init__()
    self.ViT = pretrained_model.vision_model
    self.new_position_embedding_weight = new_position_embedding_weight
    # 插值vit embeddings
    # self.interpolate_embeddings()
    self.load_interpolate_embeddings()

    self.freeze = freeze
    if self.freeze:
      for param in self.ViT.parameters():
        param.requires_grad = False
    else:
      for param in self.ViT.post_layernorm.parameters():
        param.requires_grad = False

  def forward(self, x):
    x = self.ViT(x)
    return x 

  def unfreeze(self):
    for param in self.ViT.parameters():
      param.requires_grad = True
    for param in self.ViT.post_layernorm.parameters():
      param.requires_grad = False
    # 使用siglip时需要
    # for param in self.ViT.head.parameters(): # unused
    #       param.requires_grad = False
  

  
  
  def load_interpolate_embeddings(self):
    with torch.no_grad():
      self.ViT.embeddings.position_embedding.weight.copy_(self.new_position_embedding_weight)

  def interpolate_embeddings(self):
    self.ViT.embeddings.position_ids = torch.arange(1025).unsqueeze(0)
    position_embedding = self.ViT.embeddings.position_embedding.weight
    cls_pos,ori_pos = position_embedding[0], position_embedding[1:]
    ori_pos = ori_pos.view(16,16,-1).unsqueeze(0).permute(0,3,1,2)

    resized_pos = F.interpolate(ori_pos, size=(32, 32), mode='bilinear')
    resized_pos = resized_pos.squeeze(0).permute(1,2,0).view(1024,-1)
    new_position_embedding_weight = torch.cat((cls_pos.unsqueeze(0),resized_pos),dim=0)
    new_position_embedding = nn.Embedding(1025, 1024)
    with torch.no_grad():
      new_position_embedding.weight.copy_(new_position_embedding_weight)
    self.ViT.embeddings.position_embedding = new_position_embedding

class TextEmbedding(nn.Module):
  def __init__(self, pretrained_model, freeze):
    super(TextEmbedding, self).__init__()

    # AltCLIP
    self.text_embedding = pretrained_model.text_model.roberta.embeddings
    self.freeze = freeze
    if self.freeze:
      for param in self.text_embedding.parameters():
        param.requires_grad = False

  def forward(self, x):
    x = self.text_embedding(x)
    return x

  def unfreeze(self):
    for param in self.text_embedding.parameters():
      param.requires_grad = True



class LayerPair(nn.Module):
    def __init__(self, dim, hidden_dim):
        super(LayerPair, self).__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attention = Residual(Attention(dim))
        self.norm2 = nn.LayerNorm(dim)
        self.feed_forward = Residual(FeedForward(dim, hidden_dim))

    def forward(self, x):
        x = self.norm1(x)
        x = self.attention(x)
        x = self.norm2(x)
        x = self.feed_forward(x)
        return x



def func_freeze(model):
    for param in model.parameters():
      param.requires_grad = False
    return model


class SelfAttention(nn.Module):
  def __init__(self, num_layers=6, dim=768, hidden_dim=2048):
    super(SelfAttention, self).__init__()
    self.layers = nn.ModuleList([LayerPair(dim, hidden_dim) for _ in range(num_layers)])
    self.norm = nn.LayerNorm(dim)

  def forward(self, x):
    for layer in self.layers:
      x = layer(x)
    return self.norm(x)

class HeatmapPredictor(nn.Module):
  def __init__(self, conv_info = [768, 384, 384], deconv_info=[384, 768, 384, 384, 192]):
    super(HeatmapPredictor, self).__init__()
    self.filter_size = deconv_info
    self.conv_layers = nn.Sequential(
      nn.Conv2d(in_channels=conv_info[0], out_channels=conv_info[1], kernel_size=(3, 3), stride=(1, 1), padding=1),
      nn.LayerNorm([conv_info[1], 32, 32]),
      nn.ReLU(),
      nn.Conv2d(in_channels=conv_info[1], out_channels=conv_info[2], kernel_size=(3, 3), stride=(1, 1), padding=1),
      nn.LayerNorm([conv_info[2], 32, 32]),
      nn.ReLU()
    )
    self.deconv_layers = nn.ModuleList([
      nn.ModuleList([
        nn.ConvTranspose2d(in_channels=self.filter_size[i],
                           out_channels=self.filter_size[i+1],
                           kernel_size=(3, 3),
                           stride=(2, 2),
                           padding=1, output_padding=1),
        nn.LayerNorm([self.filter_size[i+1], 32*2**(i+1), 32*2**(i+1)]),
        nn.ReLU(),
        nn.Conv2d(in_channels=self.filter_size[i+1],
                  out_channels=self.filter_size[i+1],
                  kernel_size=(3, 3), stride=(1, 1), padding=1),
        nn.LayerNorm([self.filter_size[i+1], 32*2**(i+1), 32*2**(i+1)]),
        nn.ReLU(),
        nn.Conv2d(in_channels=self.filter_size[i+1],
                  out_channels=self.filter_size[i+1],
                  kernel_size=(3, 3), stride=(1, 1), padding=1),
        nn.LayerNorm([self.filter_size[i+1], 32*2**(i+1), 32*2**(i+1)]),
        nn.ReLU(),
      ]) for i in range(len(self.filter_size)-1)
    ])
    self.final_layers = nn.Sequential(
      nn.Conv2d(in_channels=self.filter_size[-1], out_channels=1, kernel_size=(3, 3), stride=(1, 1), padding=1),
      nn.Sigmoid()
    )

  def forward(self, x):
    x = self.conv_layers(x)
    for layer in self.deconv_layers:
      for deconv in layer:
        x = deconv(x)
    x = self.final_layers(x)
    return x

class ScorePredictor(nn.Module):
    def __init__(self, filter_info=[768, 768, 384, 128, 64]):
        super(ScorePredictor, self).__init__()
        self.filter_size = filter_info
        self.conv_layers = nn.ModuleList([
            nn.ModuleList([
                nn.Conv2d(in_channels=self.filter_size[i],
                          out_channels=self.filter_size[i + 1],
                          kernel_size=(2, 2), stride=(1, 1)),
                nn.LayerNorm([self.filter_size[i + 1], 32-(i+1), 32-(i+1)]),
                nn.ReLU(),
            ]) for i in range(len(self.filter_size) - 1)
        ])
        self.flatten_size = self.filter_size[-1] * (32-(len(self.filter_size)-1))**2
        self.fc_layers = nn.Sequential(
            nn.Linear(in_features=self.flatten_size, out_features=2048),
            nn.ReLU(),
            nn.Linear(in_features=2048, out_features=1024),
            nn.ReLU(),
            nn.Linear(in_features=1024, out_features=1),
            nn.Sigmoid()
        )
        self.fc_class = nn.Sequential(
            nn.Linear(in_features=self.flatten_size, out_features=2048),
            nn.ReLU(),
            nn.Linear(in_features=2048, out_features=1024),
            nn.ReLU(),
            nn.Linear(in_features=1024, out_features=5)
        )

    def forward(self, x):
        for layers in self.conv_layers:
            for layer in layers:
                x = layer(x)
        x = torch.flatten(x, start_dim=1)
        return self.fc_layers(x),self.fc_class(x)
        #return self.fc_layers(x)
        




class RAHF(nn.Module):
  def __init__(self, freeze,model=None):
    super(RAHF, self).__init__()
    
    import sys
    sys.path.append("/home/test/junlin/Eval/EVA-CLIP/rei")
    from eva_clip import create_model_and_transforms, get_tokenizer
    model_name = "EVA02-CLIP-L-14-336"  
    pretrained = "/home/test/junlin/Eval/EVA-CLIP/rei/eva_clip/EVA02_CLIP_L_336_psz14_s6B.pt" # or "/path/to/EVA02_CLIP_B_psz16_s8B.pt"
    model, _, preprocess = create_model_and_transforms(model_name, pretrained, force_custom_clip=True)
    tokenizer = get_tokenizer(model_name)
    self.resnet = resnet50(num_classes=1024)
    
    if model is None:
      model, _, preprocess = create_model_and_transforms(model_name, pretrained, force_custom_clip=True)
    else:
      pass

    # interpolate = True if 'altclip' in pretrained_model_path else False
    #pretrained_config = AutoConfig.from_pretrained(pretrained_model_path)
    #pretrained_config.vision_config.image_size = 448
    #pretrained_model = AutoModel.from_pretrained(pretrained_model_path, config=pretrained_config, ignore_mismatched_sizes=True)
    #new_position_embedding_weight = torch.load(f"{pretrained_model_path}/new_position_embedding_weight_448.pt")
    # self.image_encoder = VisionTransformer(pretrained_model, freeze, new_position_embedding_weight)  
    # self.text_encoder = TextEmbedding(pretrained_model, freeze)          
    self.image_encoder = func_freeze(model.visual)
    self.text_encoder = func_freeze(model.text)
    #self.resnet = 
    self.self_attention = SelfAttention(dim=1024, hidden_dim=2048)       
    self.heatmap_predictor = HeatmapPredictor(conv_info = [1024, 512, 512], deconv_info=[512, 1024, 512, 512, 256])
    self.score_predictor = ScorePredictor(filter_info=[1024, 1024, 512, 128, 64])
    self.linear = nn.Linear(768, 1024)
    
    self.donw_fc = nn.Linear(2048, 1024)

  def forward(self, image, prompt, need_score=True, need_cls=False):
    #print(image.shape)
    image_token = self.image_encoder.forward_features(image, True)     # torch.Size([1, 1025, 1024])    1+32*32
    text_token = self.text_encoder(prompt)    # torch.Size([1, 512, 1024])
    text_token = self.linear(text_token)
    res_token = self.resnet(image) # # torch.Size([1, 1025, 1024])
    
    image_token = self.donw_fc(torch.cat([image_token, res_token],dim=-1))
    
    x = torch.cat([image_token, text_token], dim=1)                # torch.Size([1, 1537, 1024]) 
    x = self.self_attention(x)
    feature_map = x[:, 1:1025, :].view(-1, 32, 32, 1024).permute(0, 3, 1, 2)
    heatmap = self.heatmap_predictor(feature_map)
    #heatmap = F.interpolate(heatmap, size=(512, 512), mode='bilinear', align_corners=False)

    # if not need_score:
    #   return heatmap
    # else:
    #   score = self.score_predictor(feature_map)
    #   return heatmap, score
    
    if need_cls:
       score_pre,score_clss = self.score_predictor(feature_map)
       return heatmap,score_pre,score_clss
    else:
       score_pre,score_clss = self.score_predictor(feature_map)
       #score_pre = self.score_predictor(feature_map)
       return heatmap,score_pre   
    
if __name__ == "__main__":
    model = RAHF()